{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свёрточные нейросети и POS-теггинг\n",
    "\n",
    "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:42:57.976431Z",
     "start_time": "2019-10-29T19:42:57.959538Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyconll\n",
    "!pip install spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:34.549739Z",
     "start_time": "2019-10-29T19:49:32.179692Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "# import dlnlputils\n",
    "# from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "#     character_tokenize, pos_corpus_to_tensor, POSTagger\n",
    "# from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "\n",
    "def init_random_seed(value=0):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:08.433599Z",
     "start_time": "2019-10-29T19:46:05.110693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: невозможно создать каталог «datasets»: Файл существует\n",
      "--2022-05-18 00:02:32--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 40736565 (39M) [text/plain]\n",
      "Сохранение в: «./datasets/ru_syntagrus-ud-train.conllu»\n",
      "\n",
      "./datasets/ru_synta 100%[===================>]  38,85M  6,34MB/s    за 8,6s    \n",
      "\n",
      "2022-05-18 00:02:41 (4,51 MB/s) - «./datasets/ru_syntagrus-ud-train.conllu» сохранён [40736565/40736565]\n",
      "\n",
      "--2022-05-18 00:02:41--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14704579 (14M) [text/plain]\n",
      "Сохранение в: «./datasets/ru_syntagrus-ud-dev.conllu»\n",
      "\n",
      "./datasets/ru_synta 100%[===================>]  14,02M  10,7MB/s    за 1,3s    \n",
      "\n",
      "2022-05-18 00:02:43 (10,7 MB/s) - «./datasets/ru_syntagrus-ud-dev.conllu» сохранён [14704579/14704579]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "!mkdir datasets\n",
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.525561Z",
     "start_time": "2019-10-29T19:49:37.315213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.548127Z",
     "start_time": "2019-10-29T19:49:56.527559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.916262Z",
     "start_time": "2019-10-29T19:49:56.549806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 194\n",
      "Наибольшая длина токена 31\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:57.251433Z",
     "start_time": "2019-10-29T19:49:56.919818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.124148Z",
     "start_time": "2019-10-29T19:49:57.254191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов 142\n",
      "[('<PAD>', 0), (' ', 1), ('о', 2), ('е', 3), ('а', 4), ('т', 5), ('и', 6), ('н', 7), ('.', 8), ('с', 9)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "train_char_tokenized = tokenize_corpus(all_train_texts, \n",
    "                                       tokenizer=character_tokenize)\n",
    "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, \n",
    "                                             max_doc_freq=1.0, \n",
    "                                             min_count=5, \n",
    "                                             pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(char_vocab))\n",
    "print(list(char_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. часть речи зависит от структуры слова - будем использовать отдельные символьны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.524125Z",
     "start_time": "2019-10-29T19:49:58.125577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<NOTAG>': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'AUX': 4,\n",
       " 'CCONJ': 5,\n",
       " 'DET': 6,\n",
       " 'INTJ': 7,\n",
       " 'NOUN': 8,\n",
       " 'NUM': 9,\n",
       " 'PART': 10,\n",
       " 'PRON': 11,\n",
       " 'PROPN': 12,\n",
       " 'PUNCT': 13,\n",
       " 'SCONJ': 14,\n",
       " 'SYM': 15,\n",
       " 'VERB': 16,\n",
       " 'X': 17}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyconll.unit.conll.Conll at 0x7f95d9941400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.752672Z",
     "start_time": "2019-10-29T19:49:58.526431Z"
    }
   },
   "outputs": [],
   "source": [
    "def pos_corpus_to_tensor(sentences, char2id, label2id, max_sent_len, max_token_len):\n",
    "    inputs = torch.zeros((len(sentences), max_sent_len, max_token_len + 2), dtype=torch.long) # +2 для обозначения начала и конца слова\n",
    "    targets = torch.zeros((len(sentences), max_sent_len), dtype=torch.long)\n",
    "\n",
    "    for sent_i, sent in enumerate(sentences):\n",
    "        for token_i, token in enumerate(sent):\n",
    "            targets[sent_i, token_i] = label2id.get(token.upos, 0)\n",
    "            if token.form is None:\n",
    "                continue\n",
    "            for char_i, char in enumerate(token.form):\n",
    "                inputs[sent_i, token_i, char_i + 1] = char2id.get(char, 0) # с учётом стартового нуля для обозначения начала токена\n",
    "\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, \n",
    "                                                  MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels) # принимает на вход список тензоров - символы с метками\n",
    "\n",
    "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, \n",
    "                                                MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые пять слов второго предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.754883Z",
     "start_time": "2019-10-29T19:49:40.582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 38,  4, 25,  4, 11, 19,  7,  6, 13,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  2, 23, 11,  4,  9,  5,  7,  2, 22,  2,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 17, 16, 10,  4, 12, 11,  3,  7,  6, 20,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  9, 12, 20, 21,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 40,  3, 15,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.756496Z",
     "start_time": "2019-10-29T19:49:40.711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  1,  8,  8, 12])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательная свёрточная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.316516Z",
     "start_time": "2019-10-29T19:46:17.539Z"
    }
   },
   "outputs": [],
   "source": [
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            conv_block = nn.Sequential(\n",
    "                conv_layer(\n",
    "                    features_num, \n",
    "                    features_num, \n",
    "                    kernel_size, \n",
    "                    padding=kernel_size//2 # не меняет размер выхода\n",
    "                ),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "            layers.append(conv_block)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers: # ResNet структура\n",
    "            x = x + layer(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне отдельных токенов\n",
    "\n",
    "Без учёта информации из соседних токенов.\n",
    "Идея - рассмотреть с помощью SkipConnection разные n-граммы символов слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module): \n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        #выделение каждого токена в отдельный батч\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len) # (BatchSize*MaxSentenceLen) x MaxTokenLen\n",
    "        \n",
    "        # получение эмбеддингов символов\n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # (BatchSize*MaxSentenceLen) x MaxTokenLen x EmbSize\n",
    "        # смена размерности для подачи в сеть - размер эмбеддинга является аналогом числа каналов в изображении\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # (BatchSize*MaxSentenceLen) x EmbSize x MaxTokenLen\n",
    "        \n",
    "        features = self.backbone(char_embeddings)\n",
    "        \n",
    "        # получение пулинга от всех символов токена\n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "        \n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        \n",
    "        # обратное разделение предложений\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.318497Z",
     "start_time": "2019-10-29T19:46:23.764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 47314\n"
     ]
    }
   ],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), \n",
    "                                          embedding_size=64, layers_n=3, \n",
    "                                          kernel_size=3, dropout=0.3)\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.319470Z",
     "start_time": "2019-10-29T19:46:25.552Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Эпоха 0\n",
      "Эпоха: 384 итераций, 12.50 сек\n",
      "Среднее значение функции потерь на обучении 0.09543518974775604\n",
      "Среднее значение функции потерь на валидации 0.03880157876796651\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 384 итераций, 12.47 сек\n",
      "Среднее значение функции потерь на обучении 0.030673151505955804\n",
      "Среднее значение функции потерь на валидации 0.030709311015682646\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 384 итераций, 12.88 сек\n",
      "Среднее значение функции потерь на обучении 0.02640804090333404\n",
      "Среднее значение функции потерь на валидации 0.027459877533930362\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 384 итераций, 12.43 сек\n",
      "Среднее значение функции потерь на обучении 0.024159870682827506\n",
      "Среднее значение функции потерь на валидации 0.02633651292486356\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 384 итераций, 12.00 сек\n",
      "Среднее значение функции потерь на обучении 0.022824100757134147\n",
      "Среднее значение функции потерь на валидации 0.026661730186995303\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 384 итераций, 11.91 сек\n",
      "Среднее значение функции потерь на обучении 0.02206717881199438\n",
      "Среднее значение функции потерь на валидации 0.02345153955871811\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 384 итераций, 11.84 сек\n",
      "Среднее значение функции потерь на обучении 0.02148160747795676\n",
      "Среднее значение функции потерь на валидации 0.026731302924823053\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 384 итераций, 11.83 сек\n",
      "Среднее значение функции потерь на обучении 0.020753901978120364\n",
      "Среднее значение функции потерь на валидации 0.022890382492453745\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 384 итераций, 11.84 сек\n",
      "Среднее значение функции потерь на обучении 0.020148293134601165\n",
      "Среднее значение функции потерь на валидации 0.024280000450366206\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 384 итераций, 11.84 сек\n",
      "Среднее значение функции потерь на обучении 0.019742313987080706\n",
      "Среднее значение функции потерь на валидации 0.02252790585418444\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def copy_data_to_device(data, device):\n",
    "    if torch.is_tensor(data):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [copy_data_to_device(elem, device) for elem in data]\n",
    "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n",
    "\n",
    "\n",
    "def print_grad_stats(model):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    norm = 1e-5\n",
    "    for param in model.parameters():\n",
    "        grad = getattr(param, 'grad', None)\n",
    "        if grad is not None:\n",
    "            mean += grad.data.abs().mean()\n",
    "            std += grad.data.std()\n",
    "            norm += 1\n",
    "    mean /= norm\n",
    "    std /= norm\n",
    "    print(f'Mean grad {mean}, std {std}, n {norm}')\n",
    "\n",
    "\n",
    "def train_eval_loop(model, \n",
    "                    train_dataset, \n",
    "                    val_dataset, \n",
    "                    criterion,\n",
    "                    lr=1e-4, \n",
    "                    epoch_n=10, \n",
    "                    batch_size=32,\n",
    "                    device=None, \n",
    "                    early_stopping_patience=10, \n",
    "                    l2_reg_alpha=0,\n",
    "                    max_batches_per_epoch_train=10000,\n",
    "                    max_batches_per_epoch_val=1000,\n",
    "                    data_loader_ctor=DataLoader,\n",
    "                    optimizer_ctor=None,\n",
    "                    lr_scheduler_ctor=None,\n",
    "                    shuffle_train=True,\n",
    "                    dataloader_workers_n=0):\n",
    "    \"\"\"\n",
    "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
    "    :param model: torch.nn.Module - обучаемая модель\n",
    "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
    "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
    "    :param criterion: функция потерь для настройки модели\n",
    "    :param lr: скорость обучения\n",
    "    :param epoch_n: максимальное количество эпох\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
    "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
    "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
    "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
    "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
    "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
    "        (по умолчанию torch.utils.data.DataLoader)\n",
    "    :return: кортеж из двух элементов:\n",
    "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
    "        - лучшая модель\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # перенос модели на устройство вычисления\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # создение оптимизатора\n",
    "    if optimizer_ctor is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                     lr=lr, \n",
    "                                     weight_decay=l2_reg_alpha)\n",
    "    else:\n",
    "        optimizer = optimizer_ctor(model.parameters(), \n",
    "                                   lr=lr)\n",
    "\n",
    "    # настройка изменения скорости обучения\n",
    "    if lr_scheduler_ctor is not None:\n",
    "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    # создаём dataloader - класс pytorch, позволяющий в многопоточном режиме собирать датасеты\n",
    "    train_dataloader = data_loader_ctor(train_dataset, \n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=shuffle_train,\n",
    "                                        num_workers=dataloader_workers_n)\n",
    "    \n",
    "    val_dataloader = data_loader_ctor(val_dataset, \n",
    "                                      batch_size=batch_size, \n",
    "                                      shuffle=False,\n",
    "                                      num_workers=dataloader_workers_n)\n",
    "    \n",
    "    # инициализация сохранения лучших параметров \n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_i = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    # цикл обучения\n",
    "    for epoch_i in range(epoch_n):\n",
    "        try:\n",
    "            epoch_start = datetime.datetime.now()\n",
    "            print('Эпоха {}'.format(epoch_i))\n",
    "\n",
    "            model.train() # перевод модели в режим обучения, необх. для корр. dropout и batchnorm\n",
    "\n",
    "            mean_train_loss = 0\n",
    "            train_batches_n = 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "                # указанное количество шагов в эпоху\n",
    "                if batch_i > max_batches_per_epoch_train:\n",
    "                    break\n",
    "\n",
    "                # копирование данных на устройство модели\n",
    "                batch_x = copy_data_to_device(batch_x, device)\n",
    "                batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                pred = model(batch_x) # прямой проход\n",
    "                loss = criterion(pred, batch_y) # критерий ошибки\n",
    "\n",
    "                model.zero_grad() # очистка градиентов с прошлого шага\n",
    "                loss.backward() # находим новые градиенты\n",
    "                optimizer.step() # делаем градиентный шаг\n",
    "\n",
    "                mean_train_loss += float(loss)\n",
    "                train_batches_n += 1\n",
    "\n",
    "            mean_train_loss /= train_batches_n\n",
    "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
    "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
    "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
    "\n",
    "            \n",
    "            ### Валидация\n",
    "\n",
    "            model.eval() # перевод модели в режим предсказания\n",
    "            mean_val_loss = 0\n",
    "            val_batches_n = 0\n",
    "\n",
    "            with torch.no_grad(): # выключение сохранения градиентов - экономит ресурсы\n",
    "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
    "                    if batch_i > max_batches_per_epoch_val:\n",
    "                        break\n",
    "\n",
    "                    batch_x = copy_data_to_device(batch_x, device)\n",
    "                    batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                    pred = model(batch_x)\n",
    "                    loss = criterion(pred, batch_y)\n",
    "\n",
    "                    mean_val_loss += float(loss)\n",
    "                    val_batches_n += 1\n",
    "\n",
    "            mean_val_loss /= val_batches_n\n",
    "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
    "\n",
    "            # сохранение лучшей модели\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_epoch_i = epoch_i\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print('Новая лучшая модель!')\n",
    "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
    "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
    "                    early_stopping_patience))\n",
    "                break\n",
    "\n",
    "            # если задано расписание - обновить lr\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(mean_val_loss)\n",
    "\n",
    "            print()\n",
    "        \n",
    "        # обработка исключений\n",
    "        except KeyboardInterrupt:\n",
    "            print('Досрочно остановлено пользователем')\n",
    "            break\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
    "            break\n",
    "\n",
    "    return best_val_loss, best_model\n",
    "\n",
    "(best_val_loss,\n",
    " best_single_token_model) = train_eval_loop(single_token_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            F.cross_entropy,\n",
    "                                            lr=5e-3,\n",
    "                                            epoch_n=10,\n",
    "                                            batch_size=64,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=5,\n",
    "                                            max_batches_per_epoch_train=500,\n",
    "                                            max_batches_per_epoch_val=100,\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                       factor=0.5,\n",
    "                                                                                                                       verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.320568Z",
     "start_time": "2019-10-29T19:46:47.579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_single_token_model.state_dict(), './models/single_token_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.321566Z",
     "start_time": "2019-10-29T19:46:47.731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "single_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.324276Z",
     "start_time": "2019-10-29T19:46:48.445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [00:04, 157.38it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.019462604075670242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.96      0.88      0.92     42552\n",
      "         ADP       1.00      0.99      0.99     39344\n",
      "         ADV       0.79      0.95      0.87     22448\n",
      "         AUX       0.86      0.72      0.79      3531\n",
      "       CCONJ       0.88      0.97      0.93     15168\n",
      "         DET       0.76      0.92      0.83     10780\n",
      "        INTJ       0.00      0.00      0.00        50\n",
      "        NOUN       0.98      0.94      0.96    103538\n",
      "         NUM       0.95      0.92      0.93      6764\n",
      "        PART       0.97      0.78      0.86     13540\n",
      "        PRON       0.95      0.77      0.85     18732\n",
      "       PROPN       0.76      0.97      0.85     14858\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.80      0.73      0.76      8022\n",
      "         SYM       1.00      0.99      0.99       420\n",
      "        VERB       0.90      0.97      0.94     47753\n",
      "           X       0.98      0.65      0.78       189\n",
      "\n",
      "    accuracy                           0.99   4756104\n",
      "   macro avg       0.86      0.84      0.85   4756104\n",
      "weighted avg       0.99      0.99      0.99   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [00:01, 159.41it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.02406851202249527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1574439\n",
      "         ADJ       0.94      0.85      0.89     14793\n",
      "         ADP       1.00      0.99      0.99     13717\n",
      "         ADV       0.76      0.94      0.84      7714\n",
      "         AUX       0.88      0.68      0.77      1391\n",
      "       CCONJ       0.90      0.97      0.93      5672\n",
      "         DET       0.76      0.87      0.81      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.97      0.92      0.94     36238\n",
      "         NUM       0.90      0.90      0.90      2119\n",
      "        PART       0.96      0.76      0.85      5117\n",
      "        PRON       0.94      0.78      0.85      7444\n",
      "       PROPN       0.74      0.96      0.83      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.77      0.68      0.72      2859\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.88      0.97      0.92     17117\n",
      "           X       0.96      0.61      0.75       134\n",
      "\n",
      "    accuracy                           0.99   1727764\n",
      "   macro avg       0.85      0.82      0.83   1727764\n",
      "weighted avg       0.99      0.99      0.99   1727764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def predict_with_model(model, \n",
    "                       dataset, \n",
    "                       device=None, \n",
    "                       batch_size=32, \n",
    "                       num_workers=0, \n",
    "                       return_labels=False):\n",
    "    \"\"\"\n",
    "    :param model: torch.nn.Module - обученная модель\n",
    "    :param dataset: torch.utils.data.Dataset - данные для применения модели\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :return: numpy.array размерности len(dataset) x *\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False, \n",
    "                            num_workers=num_workers)\n",
    "    \n",
    "    results_by_batch = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in tqdm.tqdm(dataloader, \n",
    "                                          total=len(dataset)/batch_size):\n",
    "            batch_x = copy_data_to_device(batch_x, device)\n",
    "\n",
    "            if return_labels:\n",
    "                labels.append(batch_y.numpy())\n",
    "\n",
    "            batch_pred = model(batch_x)\n",
    "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
    "\n",
    "    if return_labels:\n",
    "        return (np.concatenate(results_by_batch, axis=0), \n",
    "                np.concatenate(labels, axis=0))\n",
    "    else:\n",
    "        return np.concatenate(results_by_batch, axis=0)\n",
    "\n",
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне предложений (с учётом контекста)\n",
    "\n",
    "Позволяет учитывать частиречную омонимию.\n",
    "Идея - на первом этапе проанализировать структуру слова, а потом учесть контекст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.325744Z",
     "start_time": "2019-10-29T19:46:50.139Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, \n",
    "                 single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        # первая часть - совпадает, получаем вектора токенов\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        # вторая часть - работаем на уровне токенов для учёта контекста\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.326925Z",
     "start_time": "2019-10-29T19:46:50.310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество параметров 84370\n"
     ]
    }
   ],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.327888Z",
     "start_time": "2019-10-29T19:46:50.737Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Эпоха 0\n",
      "Эпоха: 384 итераций, 12.58 сек\n",
      "Среднее значение функции потерь на обучении 0.08636830801939747\n",
      "Среднее значение функции потерь на валидации 0.030445895166975438\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 1\n",
      "Эпоха: 384 итераций, 12.70 сек\n",
      "Среднее значение функции потерь на обучении 0.027479111403711915\n",
      "Среднее значение функции потерь на валидации 0.021583563989341848\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 2\n",
      "Эпоха: 384 итераций, 12.60 сек\n",
      "Среднее значение функции потерь на обучении 0.022410835658471722\n",
      "Среднее значение функции потерь на валидации 0.02046697088839984\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 3\n",
      "Эпоха: 384 итераций, 12.57 сек\n",
      "Среднее значение функции потерь на обучении 0.020179432998702396\n",
      "Среднее значение функции потерь на валидации 0.017994869943007384\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 4\n",
      "Эпоха: 384 итераций, 12.81 сек\n",
      "Среднее значение функции потерь на обучении 0.018597769439414453\n",
      "Среднее значение функции потерь на валидации 0.015888662160475655\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 5\n",
      "Эпоха: 384 итераций, 13.15 сек\n",
      "Среднее значение функции потерь на обучении 0.017751692549306124\n",
      "Среднее значение функции потерь на валидации 0.01630933069172177\n",
      "\n",
      "Эпоха 6\n",
      "Эпоха: 384 итераций, 12.70 сек\n",
      "Среднее значение функции потерь на обучении 0.017044735182328925\n",
      "Среднее значение функции потерь на валидации 0.01478509359651863\n",
      "Новая лучшая модель!\n",
      "\n",
      "Эпоха 7\n",
      "Эпоха: 384 итераций, 12.69 сек\n",
      "Среднее значение функции потерь на обучении 0.016429968348044593\n",
      "Среднее значение функции потерь на валидации 0.015298664094972433\n",
      "\n",
      "Эпоха 8\n",
      "Эпоха: 384 итераций, 12.85 сек\n",
      "Среднее значение функции потерь на обучении 0.01613557505091497\n",
      "Среднее значение функции потерь на валидации 0.015478840199216167\n",
      "\n",
      "Эпоха 9\n",
      "Эпоха: 384 итераций, 12.10 сек\n",
      "Среднее значение функции потерь на обучении 0.015535338131788498\n",
      "Среднее значение функции потерь на валидации 0.013203989931077945\n",
      "Новая лучшая модель!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
    "                                              train_dataset,\n",
    "                                              test_dataset,\n",
    "                                              F.cross_entropy,\n",
    "                                              lr=5e-3,\n",
    "                                              epoch_n=10,\n",
    "                                              batch_size=64,\n",
    "                                              device='cuda',\n",
    "                                              early_stopping_patience=5,\n",
    "                                              max_batches_per_epoch_train=500,\n",
    "                                              max_batches_per_epoch_val=100,\n",
    "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                         factor=0.5,\n",
    "                                                                                                                         verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.542052Z",
     "start_time": "2019-08-29T13:56:16.529110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_sentence_level_model.state_dict(), './models/sentence_level_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.564926Z",
     "start_time": "2019-08-29T13:56:16.544481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "sentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.092139Z",
     "start_time": "2019-08-29T13:56:16.567242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [00:04, 162.99it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на обучении 0.010993163101375103\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   4330443\n",
      "         ADJ       0.93      0.94      0.93     42552\n",
      "         ADP       1.00      0.99      0.99     39344\n",
      "         ADV       0.92      0.88      0.90     22448\n",
      "         AUX       0.92      0.74      0.82      3531\n",
      "       CCONJ       0.94      0.98      0.96     15168\n",
      "         DET       0.94      0.88      0.91     10780\n",
      "        INTJ       0.83      0.10      0.18        50\n",
      "        NOUN       0.97      0.97      0.97    103538\n",
      "         NUM       0.91      0.96      0.93      6764\n",
      "        PART       0.97      0.88      0.92     13540\n",
      "        PRON       0.93      0.92      0.93     18732\n",
      "       PROPN       0.91      0.98      0.94     14858\n",
      "       PUNCT       1.00      1.00      1.00     77972\n",
      "       SCONJ       0.85      0.95      0.90      8022\n",
      "         SYM       0.98      1.00      0.99       420\n",
      "        VERB       0.94      0.96      0.95     47753\n",
      "           X       1.00      0.55      0.71       189\n",
      "\n",
      "    accuracy                           1.00   4756104\n",
      "   macro avg       0.94      0.87      0.89   4756104\n",
      "weighted avg       1.00      1.00      1.00   4756104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "279it [00:01, 159.98it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение функции потерь на валидации 0.0157766230404377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     <NOTAG>       1.00      1.00      1.00   1574439\n",
      "         ADJ       0.90      0.92      0.91     14793\n",
      "         ADP       0.99      0.99      0.99     13717\n",
      "         ADV       0.89      0.86      0.88      7714\n",
      "         AUX       0.90      0.73      0.81      1391\n",
      "       CCONJ       0.95      0.97      0.96      5672\n",
      "         DET       0.94      0.82      0.88      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.96      0.96      0.96     36238\n",
      "         NUM       0.86      0.95      0.90      2119\n",
      "        PART       0.96      0.85      0.90      5117\n",
      "        PRON       0.92      0.92      0.92      7444\n",
      "       PROPN       0.87      0.97      0.92      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.84      0.95      0.89      2859\n",
      "         SYM       0.79      0.98      0.88        62\n",
      "        VERB       0.93      0.95      0.94     17117\n",
      "           X       1.00      0.38      0.55       134\n",
      "\n",
      "    accuracy                           1.00   1727764\n",
      "   macro avg       0.87      0.84      0.85   1727764\n",
      "weighted avg       1.00      1.00      1.00   1727764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение полученных теггеров и сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.105418Z",
     "start_time": "2019-08-29T13:56:42.093744Z"
    }
   },
   "outputs": [],
   "source": [
    "class POSTagger:\n",
    "    def __init__(self, model, char2id, id2label, max_sent_len, max_token_len):\n",
    "        self.model = model\n",
    "        self.char2id = char2id\n",
    "        self.id2label = id2label\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __call__(self, sentences):\n",
    "        tokenized_corpus = tokenize_corpus(sentences, min_token_size=1)\n",
    "\n",
    "        inputs = torch.zeros((len(sentences), self.max_sent_len, self.max_token_len + 2), dtype=torch.long)\n",
    "\n",
    "        for sent_i, sentence in enumerate(tokenized_corpus):\n",
    "            for token_i, token in enumerate(sentence):\n",
    "                for char_i, char in enumerate(token):\n",
    "                    inputs[sent_i, token_i, char_i + 1] = self.char2id.get(char, 0)\n",
    "\n",
    "        dataset = TensorDataset(inputs, torch.zeros(len(sentences)))\n",
    "        predicted_probs = predict_with_model(self.model, dataset)  # SentenceN x TagsN x MaxSentLen\n",
    "        predicted_classes = predicted_probs.argmax(1)\n",
    "\n",
    "        result = []\n",
    "        for sent_i, sent in enumerate(tokenized_corpus):\n",
    "            result.append([self.id2label[cls] for cls in predicted_classes[sent_i, :len(sent)]])\n",
    "        return result\n",
    "\n",
    "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, \n",
    "                                    UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, \n",
    "                                      UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.125540Z",
     "start_time": "2019-08-29T13:56:42.106771Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.148124Z",
     "start_time": "2019-08-29T13:56:42.126930Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 100.77it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-NOUN раму-NOUN\n",
      "\n",
      "косил-VERB косой-NOUN косой-NOUN косой-NOUN\n",
      "\n",
      "глокая-VERB куздра-NOUN штеко-ADV будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-NOUN\n",
      "\n",
      "ведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-ADV три-NUM будет-AUX дырка-NOUN\n",
      "\n",
      "три-NUM да-ADV три-NUM будет-AUX шесть-NUM\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.168810Z",
     "start_time": "2019-08-29T13:56:42.149698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 104.10it/s]                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мама-NOUN мыла-VERB раму-NOUN\n",
      "\n",
      "косил-VERB косой-NOUN косой-NOUN косой-NOUN\n",
      "\n",
      "глокая-ADJ куздра-NOUN штеко-NOUN будланула-VERB бокра-NOUN и-CCONJ куздрячит-VERB бокрёнка-NOUN\n",
      "\n",
      "сяпала-VERB калуша-NOUN с-ADP калушатами-NOUN по-ADP напушке-NOUN\n",
      "\n",
      "пирожки-NOUN поставлены-VERB в-ADP печь-NOUN мама-NOUN любит-VERB печь-NOUN\n",
      "\n",
      "ведро-ADV дало-VERB течь-NOUN вода-NOUN стала-VERB течь-NOUN\n",
      "\n",
      "три-NUM да-PART три-NUM будет-VERB дырка-NOUN\n",
      "\n",
      "три-NUM да-PART три-NUM будет-VERB шесть-VERB\n",
      "\n",
      "сорок-NOUN сорок-NOUN\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточный модуль своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.193140Z",
     "start_time": "2019-08-29T13:56:42.170233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
    "\n",
    "        batch_size, src_channels, sequence_len = x.shape        \n",
    "        if self.padding > 0:\n",
    "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
    "            x = torch.cat((pad, x, pad), dim=-1)\n",
    "            sequence_len = x.shape[-1]\n",
    "\n",
    "        chunks = []\n",
    "        chunk_size = sequence_len - self.kernel_size + 1\n",
    "        for offset in range(self.kernel_size):\n",
    "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
    "\n",
    "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
    "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
    "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
    "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
    "        return out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.210013Z",
     "start_time": "2019-08-29T13:56:42.194620Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:00.233326Z",
     "start_time": "2019-08-29T13:56:42.211456Z"
    }
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
    "                                                      train_dataset,\n",
    "                                                      test_dataset,\n",
    "                                                      F.cross_entropy,\n",
    "                                                      lr=5e-3,\n",
    "                                                      epoch_n=10,\n",
    "                                                      batch_size=64,\n",
    "                                                      device='cuda',\n",
    "                                                      early_stopping_patience=5,\n",
    "                                                      max_batches_per_epoch_train=500,\n",
    "                                                      max_batches_per_epoch_val=100,\n",
    "                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                                 factor=0.5,\n",
    "                                                                                                                                 verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:39.145214Z",
     "start_time": "2019-08-29T14:06:00.234936Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
